{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luizrenault/snrmodel/blob/main/ss_sim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707834ab",
      "metadata": {
        "id": "707834ab"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b577926a",
      "metadata": {
        "id": "b577926a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy import integrate\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8cb48d",
      "metadata": {
        "id": "ff8cb48d"
      },
      "source": [
        "System Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a6be44",
      "metadata": {
        "id": "68a6be44"
      },
      "outputs": [],
      "source": [
        "class ParameterInfo:\n",
        "    def __init__(self, default, variation, xlabel, description=None, xticks=None, simulate=False):\n",
        "        self.default = default\n",
        "        self.variation = variation\n",
        "        self.xlabel = xlabel\n",
        "        self.description = description\n",
        "        self.xticks = xticks\n",
        "        self.simulate = simulate\n",
        "\n",
        "parameter_infos = {}\n",
        "\n",
        "parameter_infos['meanK'] = ParameterInfo(\n",
        "    default=1.88,\n",
        "    variation=np.linspace(-10, 10, 8),\n",
        "    xlabel=\"Mean Rice factor, $\\mu_K$, in dB\",\n",
        "    description=\"Mean of Rice factor (dB) for variable K over the runs and SUs.\",\n",
        "    xticks=np.arange(-10, 11, 5),\n",
        ")\n",
        "\n",
        "parameter_infos['sdK'] = ParameterInfo(\n",
        "    default=4.13,\n",
        "    variation=np.linspace(0, 10, 8),\n",
        "    xlabel=\"Rice factor std. dev., $\\sigma_K$, in dB\",\n",
        "    description=\"Standard deviation (dB) of K over the runs and SUs.\",\n",
        "    xticks=None\n",
        ")\n",
        "\n",
        "parameter_infos['randK'] = ParameterInfo(\n",
        "    default=True,\n",
        "    variation=[True, False],\n",
        "    xlabel=\"Random Rice factor, $randK$\",\n",
        "    description=\"If randK, K is random; otherwise, K = meanK.\",\n",
        "    xticks=None\n",
        ")\n",
        "\n",
        "parameter_infos['m'] = ParameterInfo(\n",
        "    default=5,\n",
        "    variation=[2, 5, 10, 16, 20, 25],\n",
        "    xlabel=\"Number of SUs, $m$\",\n",
        "    description=\"Number of SU receivers (m = 1 for NCSS or M > 1 for CSS).\",\n",
        "    xticks=np.arange(0, 26, 5),\n",
        ")\n",
        "parameter_infos['eta'] = ParameterInfo(\n",
        "    default=2.5,\n",
        "    variation=np.linspace(1, 4, 8),\n",
        "    xlabel=\"Path loss exponent, $\\eta$\",\n",
        "    description=\"Path-loss exponent.\",\n",
        "    xticks=None\n",
        ")\n",
        "parameter_infos['n'] = ParameterInfo(\n",
        "    default=400,\n",
        "    variation=[192, 368, 544, 720, 896, 1072, 1248, 1600],\n",
        "    xlabel=\"Number of samples, $n$\",\n",
        "    description=\"Number of samples per SU.\",\n",
        "    xticks=np.arange(0, 1601, 400)\n",
        ")\n",
        "parameter_infos['SNR'] = ParameterInfo(\n",
        "    default=-5,\n",
        "    variation=np.linspace(-15, 5, 8),\n",
        "    xlabel=\"SNR in dB\",\n",
        "    description=\"Average signal-to-noise ratio over all SUs, dB.\",\n",
        "    xticks=None,\n",
        ")\n",
        "parameter_infos['rho'] = ParameterInfo(\n",
        "    default=0.5,\n",
        "    variation=[0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.99],\n",
        "    xlabel=r\"Fraction $\\rho$\",\n",
        "    description=\"Fraction of noise power variations about the mean.\",\n",
        "    xticks=np.arange(0, 1.2, 0.2),\n",
        ")\n",
        "parameter_infos['Ns'] = ParameterInfo(\n",
        "    default=15,\n",
        "    variation=np.ones(11, np.integer),\n",
        "    xlabel=\"Samples per QPSK symbol, $N_s$\",\n",
        "    description=\"Number of samples per QPSK PU symbol.\",\n",
        "    xticks=np.arange(0, 221, 20)\n",
        ")\n",
        "parameter_infos['E'] = ParameterInfo(\n",
        "    default=0.1,\n",
        "    variation=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99],\n",
        "    xlabel=\"Inequality aversion of AID, $\\epsilon$\",\n",
        "    description=\"Inequality aversion parameter of the AID.\",\n",
        "    xticks=None\n",
        ")\n",
        "parameter_infos['sigma_s'] = ParameterInfo(\n",
        "    default=7,\n",
        "    variation=np.linspace(0, 15, 8),\n",
        "    xlabel=\"Shadowing standard deviation, $\\sigma_s$\",\n",
        "    description=\"Standard deviation of the Shadowing, dB.\",\n",
        "    xticks=None\n",
        ")\n",
        "\n",
        "parameter_infos['rows'] = ParameterInfo(\n",
        "    default=50,\n",
        "    variation=[50],\n",
        "    xlabel=\"Shadowing Matrix Rows\",\n",
        "    description=\"Number of rows of the full shadowing matrix.\"\n",
        ")\n",
        "\n",
        "parameter_infos['Lambda'] = ParameterInfo(\n",
        "    default=25,\n",
        "    variation=np.linspace(1, parameter_infos['rows'].default-10, 8),\n",
        "    xlabel=\"Shadowing correlation length, $\\Lambda$\",\n",
        "    description=\"Correlation length (Lambda is a fraction of the number of rows of the full shadowing matrix).\",\n",
        "    xticks=np.arange(0, parameter_infos['rows'].default-9, 10)\n",
        ")\n",
        "\n",
        "parameter_infos['r'] = ParameterInfo(\n",
        "    default=1000,\n",
        "    variation=np.linspace(100, 1000, 8),\n",
        "    xlabel=\"Operation Area Radius, $r$ (m)\",\n",
        "    description=\"Coverage radius, meters.\",\n",
        "    xticks=np.arange(100, 1001, 100)\n",
        ")\n",
        "\n",
        "parameter_infos['xPU'] = ParameterInfo(\n",
        "    default=1000,\n",
        "    variation=np.linspace(parameter_infos['r'].default, parameter_infos['r'].default*10, 8),\n",
        "    xlabel=\"PU tx location ($x=y$) in m\",\n",
        "    description=\"x-coordinate of the PU tx, m. Equal to y-coordinate.\",\n",
        "    xticks=np.arange(0, parameter_infos['r'].default*10+1, parameter_infos['r'].default*2)\n",
        ")\n",
        "\n",
        "parameter_infos['P_txPU'] = ParameterInfo(\n",
        "    default=5,\n",
        "    variation=[5],\n",
        "    xlabel=\"PU Transmission Power, $P_txPU$ (W)\",\n",
        "    description=\"PU Transmission Power.\"\n",
        ")\n",
        "\n",
        "parameter_infos['d0'] = ParameterInfo(\n",
        "    default=1,\n",
        "    variation=[1],\n",
        "    xlabel=\"Reference distance, $d_0$ (m)\",\n",
        "    description=\"Reference distance.\"\n",
        ")\n",
        "\n",
        "parameter_infos['runs'] = ParameterInfo(\n",
        "    default=1000,\n",
        "    variation=[1000],\n",
        "    xlabel=\"Number of sensing rounds, $runs$\",\n",
        "    description=\"Number of sensing rounds to compute empirical CDFs.\"\n",
        ")\n",
        "\n",
        "parameter_infos['Pfa'] = ParameterInfo(\n",
        "    default=0.1,\n",
        "    variation=[0.1],\n",
        "    xlabel=\"False alarm probability, $P_{fa}$\",\n",
        "    description=\"Reference Pfa for threshold computation.\"\n",
        ")\n",
        "\n",
        "parameter_infos['L'] = ParameterInfo(\n",
        "    default=1,\n",
        "    variation=[1, 2, 4, 8, 16, 32, 64, 128],\n",
        "    xlabel=\"Length of moving average filter, $L$\",\n",
        "    description=\"Length of the moving average filter applied to each SU before DC offset.\"\n",
        ")\n",
        "\n",
        "parameter_infos['SDCR_dB'] = ParameterInfo(\n",
        "    default=np.inf,\n",
        "    variation=[-10, -5, 0, 5, 10, 20],\n",
        "    xlabel=\"Signal-to-DC-offset ratio (SDCR) [dB]\",\n",
        "    description=\"Residual DC offset strength relative to signal power. Higher is better.\"\n",
        ")\n",
        "\n",
        "parameter_infos['fod'] = ParameterInfo(\n",
        "    default=0,\n",
        "    variation=[0.5, 0.8, 1.0, 1.2, 1.5, 2.0],\n",
        "    xlabel=\"Overdrive factor, $f_{od}$\",\n",
        "    description=\"AGC overdrive factor controlling clipping severity at the ADC.\"\n",
        ")\n",
        "\n",
        "parameter_infos['Nq_bits'] = ParameterInfo(\n",
        "    default=0,\n",
        "    variation=[0, 1, 2, 3, 4, 5, 6, 8],\n",
        "    xlabel=\"Quantization resolution [bits]\",\n",
        "    description=\"Number of bits used for quantization of SU samples before transmission to the FC.\"\n",
        ")\n",
        "\n",
        "parameter_infos['clip_range'] = ParameterInfo(\n",
        "    default=1,\n",
        "    variation=[0.5, 0.8, 1.0, 1.2, 1.5, 2.0],\n",
        "    xlabel=\"Clipping range\",\n",
        "    description=\"Range for clipping the AGC output.\"\n",
        ")\n",
        "\n",
        "\n",
        "default_parameters = {key: parameter.default for key, parameter in parameter_infos.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c350d43d",
      "metadata": {
        "id": "c350d43d"
      },
      "source": [
        "Cognitive Radio Network Simulation Area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d65aa1d",
      "metadata": {
        "id": "8d65aa1d"
      },
      "outputs": [],
      "source": [
        "pu_pos = np.array([default_parameters['xPU'], default_parameters['xPU']])\n",
        "\n",
        "def generate_su(num_users = 10, max_distance_meters = 1000):\n",
        "    rr = np.sqrt(np.random.rand(num_users))*max_distance_meters\n",
        "    theta = np.random.rand(num_users)*2*np.pi\n",
        "    su_pos = np.array([rr*np.cos(theta), rr*np.sin(theta)]).T\n",
        "\n",
        "    return su_pos\n",
        "\n",
        "r = default_parameters['r']\n",
        "rows = default_parameters['rows'] if default_parameters['rows'] > 0 else 50\n",
        "\n",
        "plt.figure(figsize=(6, 6))  # Set the figure size\n",
        "plt.gca().add_patch(plt.Circle((0, 0), r, linestyle='--', color='gray', label='SU operation area', fill=False, linewidth=2, zorder=0))\n",
        "for y in np.linspace(-r, r, rows+1): plt.plot([-r, r], [y, y], color='black', linewidth=0.2)\n",
        "for x in np.linspace(-r, r, rows+1): plt.plot([x, x], [-r, r], color='black', linewidth=0.2)\n",
        "\n",
        "\n",
        "plt.text(-r + r/rows, -r + r/rows, f'$S_{{{1},{1}}}$', color='black', fontsize=12, ha='right', va='top', zorder=1, fontweight='bold')\n",
        "plt.text(-r + r/rows, r - r/rows, f'$S_{{{1},{50}}}$', color='black', fontsize=12, ha='right', va='bottom', zorder=1, fontweight='bold')\n",
        "plt.text(r - r/rows, -r + r/rows, f'$S_{{{50},{1}}}$', color='black', fontsize=12, ha='left', va='top', zorder=1, fontweight='bold')\n",
        "plt.text(r - r/rows, r - r/rows, f'$S_{{{50},{50}}}$', color='black', fontsize=12, ha='left', va='bottom', zorder=1, fontweight='bold')\n",
        "\n",
        "plt.scatter(-r + r/rows, -r + r/rows, color='black', marker='s', s=r/rows, zorder=2)\n",
        "plt.scatter(-r + r/rows, r - r/rows, color='black', marker='s', s=r/rows, zorder=2)\n",
        "plt.scatter(r - r/rows, -r + r/rows, color='black', marker='s', s=r/rows, zorder=2)\n",
        "plt.scatter(r - r/rows, r - r/rows, color='black', marker='s', s=r/rows, zorder=2)\n",
        "\n",
        "limit = max(r, abs(pu_pos[0]), abs(pu_pos[1]))\n",
        "\n",
        "plt.xlim(-limit * 1.2, limit * 1.2)\n",
        "plt.ylim(-limit * 1.2, limit * 1.2)\n",
        "\n",
        "plt.plot([pu_pos[0], -pu_pos[0]], [pu_pos[1], -pu_pos[1]], color='magenta', linestyle='--', linewidth=1, label='Path loss reference line')\n",
        "plt.legend(loc='lower center')\n",
        "\n",
        "\n",
        "plt.scatter(0, 0, color='black', label='Origin (0,0)', zorder=5, marker='x')\n",
        "plt.text(0, 0, '(0,0)', color='black', fontsize=12, ha='left', va='top')  # Add label near the point\n",
        "\n",
        "plt.scatter(pu_pos[0], pu_pos[1], color='blue', label='PU (1000,1000)', zorder=5, marker='^')\n",
        "plt.text(pu_pos[0], pu_pos[1], 'PU', color='blue', fontsize=12, ha='left', va='bottom')\n",
        "\n",
        "angle_rad = np.deg2rad(165)\n",
        "x_end = 1000 * np.cos(angle_rad)\n",
        "y_end = 1000 * np.sin(angle_rad)\n",
        "plt.plot([0, x_end], [0, y_end], color='red', linestyle='-', linewidth=1, label='$r$')\n",
        "plt.text(x_end / 2, y_end / 2, '$r$', color='red', fontsize=12, ha='left', va='bottom')\n",
        "\n",
        "np.random.seed(48)  # For reproducibility\n",
        "\n",
        "su_pos = generate_su(default_parameters['m'], default_parameters['r'])\n",
        "\n",
        "plt.scatter(su_pos[:,0], su_pos[:,1], color='green', zorder=5)\n",
        "\n",
        "for i, su in enumerate(su_pos, start=1):\n",
        "    plt.text(su[0], su[1], f'$SU_{i}$', color='green', fontsize=12, ha='left', va='bottom')\n",
        "\n",
        "plt.plot([pu_pos[0], su_pos[0][0]], [pu_pos[1], su_pos[0][1]], color='red', linestyle='-', linewidth=1, label='$d_2$')\n",
        "plt.text((pu_pos[0] + su_pos[0][0]) / 2, (pu_pos[1] + su_pos[0][1]) / 2, f'$d_1$', color='red', fontsize=12, ha='right', va='bottom')\n",
        "\n",
        "plt.xlabel('X-axis (m)')\n",
        "plt.ylabel('Y-axis (m)')\n",
        "plt.title('Cognitive Radio Network Simulation Area Example')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54510f2a",
      "metadata": {
        "id": "54510f2a"
      },
      "source": [
        "Primary User Signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35151987",
      "metadata": {
        "id": "35151987"
      },
      "outputs": [],
      "source": [
        "def qpsk(num_samples = 1000, samples_per_symbol = 1):\n",
        "    num_symbols = int(np.ceil(num_samples/samples_per_symbol))\n",
        "    x_int = (np.random.randint(0, 4, num_symbols) * np.ones((samples_per_symbol, num_symbols))).T.flatten()\n",
        "    x_int = x_int[:num_samples]\n",
        "    x_degrees = x_int*360/4.0 + 45\n",
        "    x_radians = x_degrees*np.pi/180.0\n",
        "    x_symbols = np.cos(x_radians) + 1j*np.sin(x_radians)\n",
        "    x_symbols = np.roll(x_symbols, np.random.randint(0, samples_per_symbol))\n",
        "    return np.matrix(x_symbols * np.sqrt(2))\n",
        "\n",
        "pu_samples = qpsk(default_parameters['n'], default_parameters['Ns']) * np.sqrt(default_parameters['P_txPU']/2)\n",
        "\n",
        "fig = plt.figure(figsize=(14, 6))\n",
        "gs = GridSpec(2, 2, width_ratios=[3, 2])\n",
        "\n",
        "# I component (top left)\n",
        "ax0 = fig.add_subplot(gs[0, 0])\n",
        "ax0.plot(np.real(pu_samples.A1), color='blue')\n",
        "ax0.set_title('PU Samples - I Component')\n",
        "ax0.set_ylabel('I (In-phase)')\n",
        "ax0.grid(True)\n",
        "\n",
        "# Q component (bottom left)\n",
        "ax1 = fig.add_subplot(gs[1, 0])\n",
        "ax1.plot(np.imag(pu_samples.A1), color='red')\n",
        "ax1.set_title('PU Samples - Q Component')\n",
        "ax1.set_ylabel('Q (Quadrature)')\n",
        "ax1.set_xlabel('Sample Index')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Constellation diagram (right, spans both rows)\n",
        "ax2 = fig.add_subplot(gs[:, 1])\n",
        "ax2.scatter(np.real(pu_samples.A1), np.imag(pu_samples.A1), color='purple', s=10, alpha=0.7)\n",
        "ax2.set_title('Constellation Diagram')\n",
        "ax2.set_xlabel('I')\n",
        "ax2.set_ylabel('Q')\n",
        "ax2.grid(True)\n",
        "\n",
        "fig.suptitle('PU Samples Example', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3146a54f",
      "metadata": {
        "id": "3146a54f"
      },
      "source": [
        "Channel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825d8e5d",
      "metadata": {
        "id": "825d8e5d"
      },
      "source": [
        "Log-distance path loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089f1f33",
      "metadata": {
        "id": "089f1f33"
      },
      "outputs": [],
      "source": [
        "def log_distance_path_loss(d, d0=1, eta=4.5):\n",
        "    return (d0 / d)**eta\n",
        "\n",
        "distances = np.linalg.norm(su_pos - pu_pos, axis = 1)\n",
        "\n",
        "path_losses = log_distance_path_loss(distances, default_parameters['d0'], default_parameters['eta'])\n",
        "\n",
        "ref_line = np.linspace(500, np.linalg.norm(pu_pos)*2, 200)\n",
        "ref_path_loss = log_distance_path_loss(ref_line, default_parameters['d0'], default_parameters['eta'])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ref_line, 10*np.log10(ref_path_loss), color='magenta', label='Path Loss over Reference Line')\n",
        "plt.scatter(distances, 10*np.log10(path_losses), color='green', label='SU Path Loss', zorder=5)\n",
        "for i, (x, y) in enumerate(zip(distances, 10*np.log10(path_losses)), start=1):\n",
        "    plt.text(x, y, f'SU_{i}', color='green', fontsize=10, ha='left', va='bottom')\n",
        "plt.xlabel('Distance from PU (m)')\n",
        "plt.ylabel('Path Loss (dB)')\n",
        "plt.title('Log-distance Path Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e567267b",
      "metadata": {
        "id": "e567267b"
      },
      "source": [
        "Shadowing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e5e9ec",
      "metadata": {
        "id": "b0e5e9ec"
      },
      "outputs": [],
      "source": [
        "def Shadowing_L_matrix(rows, lambdav):\n",
        "    # Form the distance matrix\n",
        "    [X, Y] = np.meshgrid(range(1,rows+1), range(1,rows+1));\n",
        "    distances = np.sqrt(np.square(X - np.transpose(X)) + np.square(Y - np.transpose(Y)));\n",
        "\n",
        "    # Compute the correlation matrix based on the negative-exponential model\n",
        "    correlation_matrix = np.exp(-distances / lambdav);\n",
        "\n",
        "    # Cholesky decomposition to get the lower triangular matrix\n",
        "    L = np.linalg.cholesky(correlation_matrix);\n",
        "    return L\n",
        "\n",
        "def Shadowing_matrix (rows, sigma_s, L):\n",
        "    # Generate uncorrelated Gaussian (normal) random matrix\n",
        "    uncorrelated_matrix = np.random.randn(rows,rows);\n",
        "\n",
        "\n",
        "    # Multiply the uncorrelated matrix by the Cholesky factor\n",
        "    correlated_matrix = L @ uncorrelated_matrix @ L.T;\n",
        "\n",
        "    # Rescale the matrix to have unit variance\n",
        "    correlated_matrix = sigma_s * correlated_matrix;\n",
        "    return correlated_matrix\n",
        "\n",
        "def Extract_shadowing_values(SU, correlated_matrix, r):\n",
        "    rows = np.sqrt(correlated_matrix.size);\n",
        "\n",
        "    # Normalize the SU coordinates from [-r, r] to [1, rows]\n",
        "    SU = np.array(SU)\n",
        "    SU = (SU + r) / (2*r) * (rows-1) + 1\n",
        "\n",
        "    # Round the coordinates to the nearest integer for indexing\n",
        "    SU = np.round(SU);\n",
        "\n",
        "    # Ensure the indices are within the bounds of the matrix\n",
        "    SU[SU < 1] = 1; SU[SU > rows] = rows;\n",
        "\n",
        "    SU=np.array(SU-1, dtype=np.uint32);\n",
        "\n",
        "    # Extract the values from the correlated_matrix at the specified coordinates\n",
        "    m_shadowing_values = correlated_matrix[SU[:,0], SU[:,1]];\n",
        "\n",
        "    return m_shadowing_values;\n",
        "\n",
        "default_parameters['shadow_L_matrix'] = Shadowing_L_matrix(default_parameters['rows'], default_parameters['Lambda'])\n",
        "\n",
        "shadow_matrix = Shadowing_matrix(rows, default_parameters['sigma_s'], default_parameters['shadow_L_matrix'])\n",
        "shadow_values_db = Extract_shadowing_values(su_pos, shadow_matrix, r)\n",
        "shadow_value = 10**(shadow_values_db/10)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 3D surface plot\n",
        "ax3d = fig.add_subplot(121, projection='3d')\n",
        "X, Y = np.meshgrid(np.arange(rows), np.arange(rows))\n",
        "surf = ax3d.plot_surface(X, Y, shadow_matrix, cmap='viridis', edgecolor='none')\n",
        "ax3d.set_title('Correlated Shadowing Matrix')\n",
        "ax3d.set_xlabel('X')\n",
        "ax3d.set_ylabel('Y')\n",
        "ax3d.set_zlabel('Shadowing (dB)')\n",
        "fig.colorbar(surf, ax=ax3d, shrink=0.5, aspect=10)\n",
        "\n",
        "# Histogram\n",
        "axs[1].hist(shadow_matrix.flatten(), bins=40, color='skyblue', edgecolor='black')\n",
        "axs[1].set_xlabel('Shadowing Value (dB)')\n",
        "axs[1].set_ylabel('Frequency')\n",
        "axs[1].set_title('Histogram of Shadowing Matrix Values')\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean of shadow matrix values:\", np.mean(shadow_matrix))\n",
        "print(\"Standard deviation of shadow matrix values:\", np.std(shadow_matrix))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(distances, shadow_values_db, color='green', label='Shadowing Value (dB) for SUs', zorder=5)\n",
        "for i, (d, s_db) in enumerate(zip(distances, shadow_values_db), start=1):\n",
        "    plt.text(d, s_db, f'SU_{i}', color='green', fontsize=10, ha='left', va='bottom')\n",
        "plt.xlabel('Distance from PU (m)')\n",
        "plt.ylabel('Shadowing Value (dB)')\n",
        "plt.title('Shadowing Value for Each SU')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f5b031",
      "metadata": {
        "id": "c0f5b031"
      },
      "source": [
        "Ricean Factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "923bc49d",
      "metadata": {
        "id": "923bc49d"
      },
      "outputs": [],
      "source": [
        "def Ricean_Factor(distances, meanK_dB=1.88, sdK_dB=4.13, randomK=True):\n",
        "    #Complex Rician Random variable\n",
        "    if randomK:\n",
        "        K = 10**((np.random.randn(len(distances)) * sdK_dB + meanK_dB)/10)\n",
        "    else:\n",
        "        K = 10**((np.ones(len(distances)) * meanK_dB)/10)\n",
        "\n",
        "    s = np.sqrt(K/(K+1))\n",
        "    sigma = np.sqrt(1/(2*(K+1))) #Scatter standard deviation\n",
        "\n",
        "    H = np.random.normal(s, sigma, len(distances)) + 1j*np.random.normal(0, sigma, len(distances)) ###### TODO Check imaginary component MEAN\n",
        "\n",
        "    return H\n",
        "\n",
        "H = Ricean_Factor(distances, default_parameters['meanK'], default_parameters['sdK'], default_parameters['randK'])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(distances, 20 * np.log10(np.abs(H)), color='green', label='Ricean Effect (|H|) for SUs [dB]', zorder=5)\n",
        "for i, (d, h_db) in enumerate(zip(distances, 20 * np.log10(np.abs(H))), start=1):\n",
        "    plt.text(d, h_db, f'SU_{i}', color='green', fontsize=10, ha='left', va='bottom')\n",
        "plt.xlabel('Distance from PU (m)')\n",
        "plt.ylabel('Ricean Effect Magnitude (dB)')\n",
        "plt.title('Ricean Effect for SUs [dB]')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Mean of Ricean Factor (magnitude):\", np.mean(np.abs(H)))\n",
        "print(\"Standard deviation of Ricean Factor (magnitude):\", np.std(np.abs(H)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fadef2ee",
      "metadata": {
        "id": "fadef2ee"
      },
      "source": [
        "Combined channel effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfc488b",
      "metadata": {
        "id": "1dfc488b"
      },
      "outputs": [],
      "source": [
        "# Combined complex channel effect: path loss * shadowing * Ricean fading (complex)\n",
        "\n",
        "def get_combined_channel_complex(pu_pos, su_pos, parameters):\n",
        "    distances = np.linalg.norm(su_pos - pu_pos, axis = 1)\n",
        "    path_losses = log_distance_path_loss(distances, parameters['d0'], parameters['eta'])\n",
        "    if parameters['rows'] > 0:\n",
        "        shadow_values_db = Extract_shadowing_values(su_pos, Shadowing_matrix(parameters['rows'], parameters['sigma_s'], parameters['shadow_L_matrix']), parameters['r'])\n",
        "        shadow_value = 10**(shadow_values_db/10)\n",
        "    else:\n",
        "        shadow_value = np.ones(parameters['m'])\n",
        "    H = Ricean_Factor(distances, parameters['meanK'], parameters['sdK'], parameters['randK'])\n",
        "    combined_channel_complex = np.sqrt(path_losses * shadow_value) * H\n",
        "    return combined_channel_complex\n",
        "\n",
        "combined_channel_complex = get_combined_channel_complex(pu_pos, su_pos, default_parameters)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(distances, 10 * np.log10(np.abs(combined_channel_complex)), color='green', label='Combined Complex Channel Effect Magnitude', zorder=5)\n",
        "\n",
        "# Add SU names to each point\n",
        "for i, (x, y) in enumerate(zip(distances, 10 * np.log10(np.abs(combined_channel_complex))), start=1):\n",
        "    plt.text(x, y, f'SU_{i}', color='green', fontsize=10, ha='left', va='bottom')\n",
        "\n",
        "plt.xlabel('Distance from PU (m)')\n",
        "plt.ylabel('Combined Channel Effect Magnitude (dB)')\n",
        "plt.title('Combined Complex Channel Effect Magnitude (Path Loss + Shadowing + Ricean)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "274d33ba",
      "metadata": {
        "id": "274d33ba"
      },
      "source": [
        "Expected average channel effect over the operating area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d95687",
      "metadata": {
        "id": "a5d95687"
      },
      "outputs": [],
      "source": [
        "# Define the integrand function\n",
        "def integrand(z, theta, pu_pos, parameters):\n",
        "    # print(f\"Integrating at z={z}, theta={theta}\")\n",
        "    su_pos = np.array([z * np.cos(theta), z * np.sin(theta)])\n",
        "    return log_distance_path_loss(np.linalg.norm(su_pos - pu_pos), parameters['d0'], parameters['eta']) * z\n",
        "\n",
        "def CalcRXPower(parameters):\n",
        "    # Integration limits\n",
        "    theta_min, theta_max = 0, 2 * np.pi\n",
        "    z_min, z_max = 0, parameters['r']\n",
        "\n",
        "    # Compute the integral\n",
        "    integral_value, _ = integrate.dblquad(integrand, theta_min, theta_max, lambda _: z_min, lambda _: z_max, args=(np.array([parameters['xPU'], parameters['xPU']]), parameters))\n",
        "\n",
        "    # Compute expected value of P(d)\n",
        "    E_P_d = parameters['P_txPU'] * integral_value / (np.pi * parameters['r']**2)\n",
        "\n",
        "    # Compute expected value of 10^(S/10)\n",
        "    E_10S10 = np.exp(((parameters['sigma_s']**2) * np.log(10)**2) / 200) if parameters['rows'] > 0 else 1\n",
        "\n",
        "    # Compute expected value of P_rxSU (with shadowing)\n",
        "    E_P_rxSU = E_P_d * E_10S10\n",
        "    return E_P_rxSU\n",
        "\n",
        "# Compute expected value of P_rxSU (with shadowing) via numerical integration\n",
        "default_parameters['E_P_rxSU'] = CalcRXPower(default_parameters)\n",
        "\n",
        "print(\"Expected value of received power at each SU (with shadowing):\", default_parameters['E_P_rxSU'], \"Watts\")\n",
        "print(\"Expected value of received power at each SU (with shadowing):\", 10*np.log10(default_parameters['E_P_rxSU']*1000), \"dBm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ddd0819",
      "metadata": {
        "id": "6ddd0819"
      },
      "source": [
        "Receiver noise model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97c0c65",
      "metadata": {
        "id": "f97c0c65"
      },
      "outputs": [],
      "source": [
        "default_parameters['sigma2_avg'] = default_parameters['E_P_rxSU'] / (10**(default_parameters['SNR']/10))  # Average noise power per SU to achieve the target SNR\n",
        "print(\"Average noise power per SU to achieve the target SNR:\", default_parameters['sigma2_avg'], \"Watts or \", 10*np.log10(default_parameters['sigma2_avg']*1000), \"dBm\" )\n",
        "\n",
        "\n",
        "\n",
        "def check_sigma2_avg_update_needed(parameters):\n",
        "    # Recalculate E_P_rxSU if any relevant parameters have changed\n",
        "    relevant_params = ['P_txPU', 'r', 'd0', 'eta', 'sigma_s', 'xPU']\n",
        "    if any(param in parameters for param in relevant_params):\n",
        "        parameters['E_P_rxSU'] = CalcRXPower(parameters)\n",
        "        parameters['sigma2_avg'] = parameters['E_P_rxSU'] / (10**(parameters['SNR']/10))\n",
        "        print(\"Updated Expected value of received power at each SU (with shadowing):\", parameters['E_P_rxSU'], \"Watts\")\n",
        "        print(\"Updated Average noise power per SU to achieve the target SNR:\", parameters['sigma2_avg'], \"Watts or \", 10*np.log10(parameters['sigma2_avg']*1000), \"dBm\" )\n",
        "    else:\n",
        "        print(\"No relevant parameter changes detected. No update needed.\")\n",
        "\n",
        "def noise_su(parameters):\n",
        "    U = np.random.uniform(-1, 1, parameters['m'])\n",
        "    sigma2 = (1 + parameters['rho'] * U) * parameters['sigma2_avg']\n",
        "    sqrtsigma2 = np.sqrt(sigma2/2)\n",
        "    n = np.random.normal(0, sqrtsigma2, [parameters['n'], parameters['m']]) + 1j*np.random.normal(0, sqrtsigma2, [parameters['n'], parameters['m']])\n",
        "    return n.T;\n",
        "\n",
        "noise = noise_su(default_parameters)\n",
        "\n",
        "# Received signal at first SU: channel effect * PU signal\n",
        "rx_signal = combined_channel_complex[:, np.newaxis] * pu_samples\n",
        "rx_signal_first_su = np.array(rx_signal[0] + noise[0]).flatten()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(np.real(rx_signal_first_su), np.imag(rx_signal_first_su), color='orange', s=10, alpha=0.7)\n",
        "plt.title('Constellation Diagram: First SU Received Signal + Noise')\n",
        "plt.xlabel('I')\n",
        "plt.ylabel('Q')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126f24d3",
      "metadata": {
        "id": "126f24d3"
      },
      "source": [
        "Detectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22baf70d",
      "metadata": {
        "id": "22baf70d"
      },
      "outputs": [],
      "source": [
        "E = 0.1;             # Inequality aversion parameter of the AID.\n",
        "\n",
        "def calculate_GID(x_h0, x_h1):\n",
        "    Num_h0=0\n",
        "    Num_h1=0\n",
        "    for u in range(x_h0.size):\n",
        "        for j in range(u+1, x_h0.size): # Faster with j=u:m^2 instead of j=1:m^2 (sum of (m^4 + m^2)/2 terms, instead of m^4)\n",
        "            Num_h0 = Num_h0 + np.abs(x_h0[u]-x_h0[j]);\n",
        "            Num_h1 = Num_h1 + np.abs(x_h1[u]-x_h1[j]);\n",
        "    Tgid_h0 = np.sum(np.abs(x_h0))/Num_h0;\n",
        "    Tgid_h1 = np.sum(np.abs(x_h1))/Num_h1;\n",
        "\n",
        "    return Tgid_h0[0,0], Tgid_h1[0,0]\n",
        "\n",
        "\n",
        "\n",
        "def calculate_PRIDE(x_h0, x_h1):\n",
        "    \"\"\"Calculates the Pietra-Ricci Index Detector (PRIDE) statistic.\n",
        "\n",
        "    The PRIDE statistic is a measure of inequality in a distribution.\n",
        "    It is calculated as the ratio of the sum of absolute values in the\n",
        "    distribution to the sum of absolute differences between each value\n",
        "    and the mean of the distribution.\n",
        "\n",
        "    Args:\n",
        "        x_h0: Flattened received signal sample covariance matrix under H0.\n",
        "        x_h1: Flattened received signal sample covariance matrix under H1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the PRIDE statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "    m0 = np.mean(x_h0);\n",
        "    m1 = np.mean(x_h1);\n",
        "    Tpride_h0 = np.sum(np.abs(x_h0))/np.sum(np.abs(x_h0 - m0));\n",
        "    Tpride_h1 = np.sum(np.abs(x_h1))/np.sum(np.abs(x_h1 - m1));\n",
        "    return Tpride_h0, Tpride_h1\n",
        "\n",
        "## AID (Atkinson index detector) test statistic\n",
        "def calculate_AID(R_h0, R_h1, E):\n",
        "    \"\"\"Calculates the Atkinson Index Detector (AID) statistic.\n",
        "\n",
        "    The AID statistic is a measure of inequality in a distribution.\n",
        "    It is a family of indices parameterized by E, reflecting the\n",
        "    degree of inequality aversion.\n",
        "\n",
        "    Args:\n",
        "        R_h0: Received signal sample covariance matrix under H0.\n",
        "        R_h1: Received signal sample covariance matrix under H1.\n",
        "        E: Inequality aversion parameter.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the AID statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "    m0 = np.mean(R_h0);\n",
        "    m1 = np.mean(R_h1);\n",
        "    if E==0.5:\n",
        "        SUM0=0;\n",
        "        SUM1=0;\n",
        "        for ROW in range(m):\n",
        "            for COL in range(ROW, m):\n",
        "                I=COL==ROW;\n",
        "                SUM0 = SUM0 + (2-I) * np.sqrt(np.abs(R_h0[ROW, COL]) + np.real(R_h0[ROW,COL]));\n",
        "                SUM1 = SUM1 + (2-I) * np.sqrt(np.abs(R_h1[ROW, COL]) + np.real(R_h1[ROW,COL]));\n",
        "        Taid_h0 = (SUM0**2)/m0;\n",
        "        Taid_h1 = (SUM1**2)/m1;\n",
        "    else:\n",
        "        Taid_h0 = (np.float_power((np.sum(np.sum(np.float_power(R_h0,(1-E))))),(1/(1-E))))/m0; # Corrected on Nov/2024\n",
        "        Taid_h1 = (np.float_power((np.sum(np.sum(np.float_power(R_h1,(1-E))))),(1/(1-E))))/m1; # Corrected on Nov/2024\n",
        "    return np.abs(Taid_h0), np.abs(Taid_h1)\n",
        "\n",
        "\n",
        "def calculate_HR(R_h0, R_h1):\n",
        "    \"\"\"\n",
        "    Calculates the Hadamard Ratio (HR) statistic.\n",
        "\n",
        "    Args:\n",
        "        R_h0: Received signal sample covariance matrix under H0.\n",
        "        R_h1: Received signal sample covariance matrix under H1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the HR statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "    Thr_h0 = -np.real(np.linalg.det(R_h0) / np.prod(np.diag(R_h0)))\n",
        "    Thr_h1 = -np.real(np.linalg.det(R_h1) / np.prod(np.diag(R_h1)))\n",
        "    return Thr_h0, Thr_h1\n",
        "\n",
        "def calculate_VD1(R_h0, R_h1):\n",
        "    \"\"\"\n",
        "    Calculates the Variance-Distance Ratio (VD1) statistic.\n",
        "\n",
        "    Args:\n",
        "        R_h0: Received signal sample covariance matrix under H0.\n",
        "        R_h1: Received signal sample covariance matrix under H1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the VD1 statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "\n",
        "    # Preallocate arrays for dh0 and dh1\n",
        "    M = R_h0.shape[0]\n",
        "    dh0 = np.zeros(M)\n",
        "    dh1 = np.zeros(M)\n",
        "\n",
        "    # Compute norms for each row\n",
        "    for j in range(M):\n",
        "        dh0[j] = np.linalg.norm(R_h0[j, :])\n",
        "        dh1[j] = np.linalg.norm(R_h1[j, :])\n",
        "\n",
        "    # Create diagonal matrices\n",
        "    DD_h0 = np.diag(dh0)\n",
        "    DD_h1 = np.diag(dh1)\n",
        "\n",
        "    # Compute Tvd1_h0 and Tvd1_h1\n",
        "    Tvd1_h0 = -np.real(np.log(np.linalg.det(np.linalg.inv(DD_h0) @ R_h0)))\n",
        "    Tvd1_h1 = -np.real(np.log(np.linalg.det(np.linalg.inv(DD_h1) @ R_h1)))\n",
        "\n",
        "    return Tvd1_h0, Tvd1_h1\n",
        "\n",
        "def calculate_LMPIT(R_h0, R_h1):\n",
        "    \"\"\"\n",
        "    Calculates the Locally Most Powerful Invariant Test (LMPIT) statistic.\n",
        "\n",
        "    Args:\n",
        "        R_h0: Received signal sample covariance matrix under H0.\n",
        "        R_h1: Received signal sample covariance matrix under H1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the LMPIT statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "\n",
        "    # Diagonal matrices\n",
        "    D_h0 = np.diag(np.diag(R_h0))\n",
        "    D_h1 = np.diag(np.diag(R_h1))\n",
        "\n",
        "    # Matrix normalization\n",
        "    D_h0_inv_sqrt = np.linalg.inv(np.sqrt(D_h0))\n",
        "    D_h1_inv_sqrt = np.linalg.inv(np.sqrt(D_h1))\n",
        "\n",
        "    C_h0 = D_h0_inv_sqrt @ R_h0 @ D_h0_inv_sqrt\n",
        "    C_h1 = D_h1_inv_sqrt @ R_h1 @ D_h1_inv_sqrt\n",
        "\n",
        "    # Compute Frobenius norm squared\n",
        "    Tlmpit_h0 = np.linalg.norm(C_h0, 'fro')**2\n",
        "    Tlmpit_h1 = np.linalg.norm(C_h1, 'fro')**2\n",
        "\n",
        "    return Tlmpit_h0, Tlmpit_h1\n",
        "\n",
        "def calculate_MMED(lambda_h0, lambda_h1):\n",
        "    \"\"\"\n",
        "    Calculates the Maximum-Minimum Eigenvalue Detector (MMED) statistic.\n",
        "\n",
        "    Args:\n",
        "        lambda_h0: Eigenvalues of the received signal sample covariance matrix under H0.\n",
        "        lambda_h1: Eigenvalues of the received signal sample covariance matrix under H1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the MMED statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "\n",
        "    Tmmed_h0 = lambda_h0[0]/lambda_h0[-1]\n",
        "    Tmmed_h1 = lambda_h1[0]/lambda_h1[-1]\n",
        "\n",
        "    return Tmmed_h0, Tmmed_h1\n",
        "\n",
        "\n",
        "def calculate_SLE(lambda_h0, lambda_h1):\n",
        "    \"\"\"\n",
        "    Calculates the Generalized Likelihood Ratio Test (sle) statistic.\n",
        "\n",
        "    Args:\n",
        "        lambda_h0: Eigenvalues of the received signal sample covariance matrix under H0.\n",
        "        lambda_h1: Eigenvalues of the received signal sample covariance matrix under H1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the sle statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "\n",
        "    Tsle_h0 = lambda_h0[0]/np.sum(lambda_h0[1:-1]);\n",
        "    Tsle_h1 = lambda_h1[0]/np.sum(lambda_h1[1:-1]);\n",
        "    return Tsle_h0, Tsle_h1\n",
        "\n",
        "\n",
        "def calculate_AGM(lambda_h0, lambda_h1):\n",
        "    \"\"\"\n",
        "    Calculates the Arithmetic-Geometric Mean (AGM) statistic.\n",
        "\n",
        "    Args:\n",
        "        lambda_h0: Eigenvalues of the received signal sample covariance matrix under H0.\n",
        "        lambda_h1: Eigenvalues of the received signal sample covariance matrix under H1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the AGM statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "    Tagm_h0 = np.sum(lambda_h0)/((np.prod(lambda_h0))**(1/lambda_h0.size**2));\n",
        "    Tagm_h1 = np.sum(lambda_h1)/((np.prod(lambda_h1))**(1/lambda_h1.size**2));\n",
        "    return Tagm_h0, Tagm_h1\n",
        "\n",
        "\n",
        "def calculate_MSEE(lambda_h0, lambda_h1):\n",
        "    \"\"\"\n",
        "    Calculates the Mean-to-Square Extreme Eigenvalue (MSEE) statistic.\n",
        "\n",
        "    Args:\n",
        "        lambda_h0: Eigenvalues of the received signal sample covariance matrix under H0.\n",
        "        lambda_h1: Eigenvalues of the received signal sample covariance matrix under H1.\n",
        "\n",
        "        Returns:\n",
        "        A tuple containing the MSEE statistic under H0 and H1.\n",
        "    \"\"\"\n",
        "    Tmsee_h0 = (lambda_h0[0]+lambda_h0[-1])/np.sqrt(lambda_h0[0]*lambda_h0[-1]);\n",
        "    Tmsee_h1 = (lambda_h1[0]+lambda_h1[-1])/np.sqrt(lambda_h1[0]*lambda_h1[-1]);\n",
        "    return Tmsee_h0, Tmsee_h1\n",
        "\n",
        "def calculate_TID(R_h0, R_h1):\n",
        "    m = R_h0.shape[0]\n",
        "    r_abs_h0 = np.sum(np.abs(R_h0))\n",
        "    r_abs_h1 = np.sum(np.abs(R_h1))\n",
        "    SUM0=0\n",
        "    SUM1=0\n",
        "    for ROW in range(m):\n",
        "        for COL in range(ROW, m):\n",
        "            I=COL==ROW\n",
        "            SUM0 = SUM0 + (2-I) * np.abs(R_h0[ROW, COL]) * np.log(np.abs(R_h0[ROW,COL])/r_abs_h0)\n",
        "            SUM1 = SUM1 + (2-I) * np.abs(R_h1[ROW, COL]) * np.log(np.abs(R_h1[ROW,COL])/r_abs_h1)\n",
        "    Ttid_h0 = 1/SUM0\n",
        "    Ttid_h1 = 1/SUM1\n",
        "    return Ttid_h0, Ttid_h1\n",
        "\n",
        "\n",
        "\n",
        "def calculate_metrics(rx_signal_h0, rx_signal_h1):\n",
        "    \"\"\"\n",
        "    Calculates various detection metrics based on the received signal under two hypotheses.\n",
        "\n",
        "    Args:\n",
        "        rx_signal_h0: Received signal under hypothesis H0.\n",
        "        rx_signal_h1: Received signal under hypothesis H1.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the calculated metrics.\n",
        "    \"\"\"\n",
        "    X_h0 = np.matrix(rx_signal_h0)\n",
        "    X_h1 = np.matrix(rx_signal_h1)\n",
        "\n",
        "\n",
        "    # Sample covariance matrices\n",
        "    if(X_h0.shape[1] == 1):\n",
        "        R_h0 = (X_h0 @ X_h0.conj().T) /X_h0.shape[1]  # TODO\n",
        "        R_h1 = (X_h1 @ X_h1.conj().T) / X_h1.shape[1] # TODO\n",
        "    else:\n",
        "        R_h0 = (X_h0 @ X_h0.conj().T) /X_h0.shape[1]\n",
        "        R_h1 = (X_h1 @ X_h1.conj().T) / X_h1.shape[1]\n",
        "\n",
        "    # Eigenvalues\n",
        "    lambda_h0 = np.linalg.eigvalsh(R_h0)[::-1]  # Sorted in descending order\n",
        "    lambda_h1 = np.linalg.eigvalsh(R_h1)[::-1]  # Sorted in descending order\n",
        "\n",
        "    # Flattened received signal sample covariance matrices\n",
        "    x_h0 = R_h0.flatten().reshape(-1, 1)\n",
        "    x_h1 = R_h1.flatten().reshape(-1, 1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    Tgid_h0, Tgid_h1 = calculate_GID(x_h0, x_h1)\n",
        "    Tpride_h0, Tpride_h1 = calculate_PRIDE(x_h0, x_h1)\n",
        "    Taid_h0, Taid_h1 = calculate_AID(R_h0, R_h1, E)  # Example with E=0.5\n",
        "    Thr_h0, Thr_h1 = calculate_HR(R_h0, R_h1)\n",
        "    Tvd1_h0, Tvd1_h1 = calculate_VD1(R_h0, R_h1)\n",
        "    Tlmpit_h0, Tlmpit_h1 = calculate_LMPIT(R_h0, R_h1)\n",
        "    Tmmed_h0, Tmmed_h1 = calculate_MMED(lambda_h0, lambda_h1)\n",
        "    Tsle_h0, Tsle_h1 = calculate_SLE(lambda_h0, lambda_h1)\n",
        "    Tagm_h0, Tagm_h1 = calculate_AGM(lambda_h0, lambda_h1)\n",
        "    Tmsee_h0, Tmsee_h1 = calculate_MSEE(lambda_h0, lambda_h1)\n",
        "    Ttid_h0, Ttid_h1 = calculate_TID(R_h0, R_h1)\n",
        "\n",
        "    return {\n",
        "        'GID': (Tgid_h0, Tgid_h1),\n",
        "        'PRIDE': (Tpride_h0, Tpride_h1),\n",
        "        'AID': (Taid_h0, Taid_h1),\n",
        "        'HR': (Thr_h0, Thr_h1),\n",
        "        'VD1': (Tvd1_h0, Tvd1_h1),\n",
        "        'LMPIT': (Tlmpit_h0, Tlmpit_h1),\n",
        "        'MMED': (Tmmed_h0, Tmmed_h1),\n",
        "        'SLE': (Tsle_h0, Tsle_h1),\n",
        "        'AGM': (Tagm_h0, Tagm_h1),\n",
        "        'MSEE': (Tmsee_h0, Tmsee_h1),\n",
        "        'TID': (Ttid_h0, Ttid_h1)\n",
        "    }\n",
        "\n",
        "curve_styles = {\n",
        "    'GID':   dict(fmt='c-o',  linewidth=2.5, markerfacecolor='k', markersize=7, label='GID'),\n",
        "    'PRIDE': dict(fmt='m-h',  linewidth=2.5, markerfacecolor='w', markersize=7, label='PRIDe'),\n",
        "    'AID':   dict(fmt='r-s',  linewidth=2.5, markerfacecolor='w', markersize=7, label='AID'),\n",
        "    'HR':    dict(fmt='k-o',  linewidth=2.5, markerfacecolor='w', markersize=7, label='HR'),\n",
        "    'VD1':   dict(fmt='b-^',  linewidth=2.5, markerfacecolor='w', markersize=7, label='VD1'),\n",
        "    'SLE':   dict(fmt='y-->', linewidth=2.5, markerfacecolor='b', markersize=7, label='SLE'),\n",
        "    'MMED':  dict(fmt='r--<', linewidth=2.5, markerfacecolor='w', markersize=7, label='MMED'),\n",
        "    'AGM':   dict(fmt='b--p', linewidth=2.5, markerfacecolor='w', markersize=7, label='AGM'),\n",
        "    'LMPIT': dict(fmt='m-*',  linewidth=2.5, markerfacecolor='w', markersize=7, label='LMPIT'),\n",
        "    'MSEE':  dict(fmt='k--v', linewidth=2.5, markerfacecolor='w', markersize=7, label='MSEE'),\n",
        "    'TID':   dict(fmt='g-D',  linewidth=2.5, markerfacecolor='w', markersize=7, label='TID')\n",
        "}\n",
        "\n",
        "metrics = calculate_metrics(noise, rx_signal+noise)\n",
        "for metric, values in metrics.items():\n",
        "    print(f\"{metric} - H0: {values[0]}, H1: {values[1]}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fbdd156",
      "metadata": {
        "id": "3fbdd156"
      },
      "source": [
        "Direct Conversion Receiver Effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4aab78",
      "metadata": {
        "id": "7e4aab78"
      },
      "outputs": [],
      "source": [
        "def moving_average_filter(samples, parameters):\n",
        "    if parameters['L'] <= 1:\n",
        "        return samples\n",
        "    L = parameters['L']\n",
        "    h = np.ones(L) / np.sqrt(L)\n",
        "\n",
        "    for su in range(parameters['m']):\n",
        "        samples[su] = np.convolve(np.asarray(samples[su]).flatten(), h, mode='same')[:samples.shape[1]]\n",
        "\n",
        "    return samples\n",
        "\n",
        "def whitening_matrix(parameters):\n",
        "    L = parameters['L']\n",
        "    a = np.zeros(parameters['n'])\n",
        "    a[:L] = 1 - np.arange(L)/L\n",
        "    Q = scipy.linalg.toeplitz(a)\n",
        "    L_c = np.linalg.cholesky(Q)\n",
        "    W = np.linalg.inv(L_c)  # whitening matrix\n",
        "    return W.T\n",
        "\n",
        "def whitening(samples, parameters):\n",
        "    if parameters['L'] <= 1:\n",
        "        return samples\n",
        "    samples = np.array(samples).flatten()\n",
        "    return (samples @ parameters['W_matrix'])\n",
        "\n",
        "def gen_dc_offset(parameters):\n",
        "    sigma2_dc = parameters['sigma2_dc']\n",
        "    D = np.zeros((parameters['m'], parameters['n']), dtype=complex)\n",
        "    for i in range(parameters['m']):\n",
        "        D[i,:] = np.sqrt(sigma2_dc/2)*(np.random.randn() + 1j*np.random.randn())\n",
        "    return D\n",
        "\n",
        "def agc(samples, parameters):\n",
        "\n",
        "    Y_out = np.empty_like(samples)\n",
        "    for i in range(parameters['m']):\n",
        "        g_i = parameters['fod'] * np.sqrt(2*parameters['n']) / (6*np.linalg.norm(samples[i,:]))\n",
        "        Y_out[i,:] = g_i * samples[i,:]\n",
        "    return Y_out\n",
        "\n",
        "def quantize(samples, parameters):\n",
        "    clip_range = parameters['clip_range']\n",
        "    Nq_bits = parameters['Nq_bits']\n",
        "    N = 2**Nq_bits\n",
        "    delta = 2*clip_range / N\n",
        "    def qfun(x):\n",
        "        idx = np.floor((np.clip(x, -clip_range, clip_range) + clip_range)/delta + 0.5)\n",
        "        return -clip_range + idx*delta\n",
        "    return qfun(np.real(samples)) + 1j*qfun(np.imag(samples))\n",
        "\n",
        "default_parameters['W_matrix'] = whitening_matrix(default_parameters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba71a681",
      "metadata": {
        "id": "ba71a681"
      },
      "source": [
        "Simulation Routine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f8f8ec",
      "metadata": {
        "id": "e8f8f8ec"
      },
      "outputs": [],
      "source": [
        "plot_statistics = False;    # If plot_statistics, plot the test statistics for.\n",
        "plot_histogram = False;     # If plot_histogram, plot histograms of the test statistics.\n",
        "plot_roc = False;           # If plot_roc, plot ROC curves.\n",
        "\n",
        "temp_parameters = ['shadow_L_matrix', 'E_P_rxSU', 'sigma2_avg', 'W_matrix']\n",
        "\n",
        "for param in parameter_infos.values():\n",
        "    param.simulate = False\n",
        "\n",
        "parameter_infos['meanK'].simulate = True\n",
        "#parameter_infos['SDCR_dB'].simulate = True\n",
        "#parameter_infos['L'].simulate = True\n",
        "#parameter_infos['eta'].simulate = True\n",
        "\n",
        "parameter_infos['rho'].default = 0\n",
        "# parameter_infos['r'].default = 1\n",
        "# parameter_infos['xPU'].default = 1\n",
        "# parameter_infos['d0'].default = 0.001\n",
        "\n",
        "\n",
        "# parameter_infos['m'].default = 6\n",
        "# parameter_infos['n'].default = 250\n",
        "# parameter_infos['SNR'].default = -10\n",
        "# parameter_infos['rows'].default = 0\n",
        "# parameter_infos['Ns'].default = 25\n",
        "\n",
        "# parameter_infos['SDCR_dB'].default = 5\n",
        "# parameter_infos['L'].default = 25\n",
        "# parameter_infos['fod'].default = 1.2\n",
        "# parameter_infos['Nq_bits'].default = 3\n",
        "\n",
        "def iteration(parameters):\n",
        "    pu_pos = np.array([parameters['xPU'], parameters['xPU']])\n",
        "    su_pos = generate_su(parameters['m'], parameters['r'])\n",
        "    pu_samples = qpsk(parameters['n'], parameters['Ns']) * np.sqrt(parameters['P_txPU']/2)\n",
        "\n",
        "    combined_channel_complex = get_combined_channel_complex(pu_pos, su_pos, parameters)[:, np.newaxis]\n",
        "    rx_signal = combined_channel_complex @ pu_samples\n",
        "\n",
        "    noise = noise_su(parameters)\n",
        "\n",
        "    sensored_rx_signal = rx_signal + noise\n",
        "\n",
        "    if parameters['L'] > 1:\n",
        "        sensored_rx_signal = moving_average_filter(sensored_rx_signal, parameters)\n",
        "        noise = moving_average_filter(noise, parameters)\n",
        "\n",
        "    if parameters['sigma2_dc'] > 0:\n",
        "        dc_samples = gen_dc_offset(parameters)\n",
        "        sensored_rx_signal += dc_samples\n",
        "        noise += dc_samples\n",
        "\n",
        "    if parameters['fod'] > 0:\n",
        "        sensored_rx_signal = agc(sensored_rx_signal, parameters)\n",
        "        noise = agc(noise, parameters)\n",
        "\n",
        "    if parameters['Nq_bits'] > 0:\n",
        "        sensored_rx_signal = quantize(sensored_rx_signal, parameters)\n",
        "        noise = quantize(noise, parameters)\n",
        "\n",
        "\n",
        "    if parameters['L'] > 1:\n",
        "        for su in range(parameters['m']):\n",
        "            sensored_rx_signal[su] = whitening(sensored_rx_signal[su], parameters)\n",
        "            noise[su] = whitening(noise[su], parameters)\n",
        "\n",
        "    metrics = calculate_metrics(noise, sensored_rx_signal)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "for parameter_name, parameter in parameter_infos.items():\n",
        "    if parameter.simulate:\n",
        "        print(f\"Simulating for parameter: {parameter_name}\")\n",
        "        results = {key: {'H0': [], 'H1': [], 'H0_min': [], 'H0_max': [], 'H1_min': [], 'H1_max': [], 'Pd': [], 'Threshold': []} for key in curve_styles.keys()}\n",
        "\n",
        "\n",
        "        current_parameters = {key: parameter.default for key, parameter in parameter_infos.items()}\n",
        "\n",
        "        current_parameters['E_P_rxSU'] = CalcRXPower(current_parameters)\n",
        "        current_parameters['shadow_L_matrix'] = Shadowing_L_matrix(current_parameters['rows'], current_parameters['Lambda'])\n",
        "        current_parameters['W_matrix'] = whitening_matrix(current_parameters)\n",
        "\n",
        "        for idx, value in enumerate(parameter.variation):\n",
        "            current_parameters[parameter_name] = value\n",
        "\n",
        "            print(f\"  Running with {parameter_name} = {value}\", \"    \", {k: v for k, v in current_parameters.items() if k not in [parameter_name, *temp_parameters]})\n",
        "\n",
        "            all_metrics = {key: {'H0': [], 'H1': []} for key in curve_styles.keys()}\n",
        "\n",
        "            if parameter_name in ['r', 'xPU', 'eta', 'P_txPU', 'd0', 'sigma_s']:\n",
        "                current_parameters['E_P_rxSU'] = CalcRXPower(current_parameters)\n",
        "\n",
        "            current_parameters['sigma2_avg'] = current_parameters['E_P_rxSU'] / (10**(current_parameters['SNR']/10))  # Average noise power per SU to achieve the target SNR\n",
        "            current_parameters['sigma2_dc'] = 0 if current_parameters['SDCR_dB'] == np.inf else current_parameters['E_P_rxSU'] / (10**(current_parameters['SDCR_dB']/10))\n",
        "\n",
        "            if parameter_name in ['rows', 'Lambda']:\n",
        "                current_parameters['shadow_L_matrix'] = Shadowing_L_matrix(current_parameters['rows'], current_parameters['Lambda'])\n",
        "\n",
        "            if parameter_name in ['L', 'n']:\n",
        "                current_parameters['W_matrix'] = whitening_matrix(current_parameters)\n",
        "\n",
        "\n",
        "\n",
        "            for _ in tqdm(range(current_parameters['runs'])):\n",
        "                metrics = iteration(current_parameters)\n",
        "                for key, values in metrics.items():\n",
        "                    all_metrics[key]['H0'].append(values[0])\n",
        "                    all_metrics[key]['H1'].append(values[1])\n",
        "                pass\n",
        "\n",
        "\n",
        "            for key in results.keys():\n",
        "                results[key]['H0'].append(np.mean(all_metrics[key]['H0']))\n",
        "                results[key]['H1'].append(np.mean(all_metrics[key]['H1']))\n",
        "\n",
        "                # Calculate probability of detection (Pd) for the current metric and parameter value\n",
        "                # Use the empirical distributions of H0 and H1 for thresholding at reference Pfa\n",
        "                # Find threshold for reference Pfa\n",
        "                threshold = np.percentile(np.array(all_metrics[key]['H0']), 100 * (1 - current_parameters['Pfa']))\n",
        "                # Pd: fraction of H1 samples above threshold\n",
        "                Pd = np.mean(np.array(all_metrics[key]['H1']) > threshold)\n",
        "\n",
        "                results[key]['Pd'].append(Pd)\n",
        "                results[key]['Threshold'].append(threshold)\n",
        "\n",
        "                if plot_statistics:\n",
        "                    results[key]['H0_min'].append(np.min(all_metrics[key]['H0']))\n",
        "                    results[key]['H0_max'].append(np.max(all_metrics[key]['H0']))\n",
        "                    results[key]['H1_min'].append(np.min(all_metrics[key]['H1']))\n",
        "                    results[key]['H1_max'].append(np.max(all_metrics[key]['H1']))\n",
        "\n",
        "\n",
        "            if plot_roc:\n",
        "                # Calculate ROC curves for each statistic\n",
        "                fig_roc, ax_roc = plt.subplots(figsize=(5, 4))\n",
        "                for key in results.keys():\n",
        "                    h0 = np.array(all_metrics[key]['H0'])\n",
        "                    h1 = np.array(all_metrics[key]['H1'])\n",
        "                    thresholds = np.percentile(np.array(all_metrics[key]['H0']), 100 * np.linspace(0, 1, 20))\n",
        "                    Pfa_roc = []\n",
        "                    Pd_roc = []\n",
        "                    for t in thresholds:\n",
        "                        Pfa_roc.append(np.mean(h0 > t))\n",
        "                        Pd_roc.append(np.mean(h1 > t))\n",
        "                    ax_roc.plot(Pfa_roc, Pd_roc, curve_styles[key]['fmt'], linewidth=curve_styles[key]['linewidth'],\n",
        "                                markerfacecolor=curve_styles[key]['markerfacecolor'], markersize=curve_styles[key]['markersize'],\n",
        "                                label=curve_styles[key]['label'])\n",
        "\n",
        "                ax_roc.set_xlabel('Probability of False Alarm (Pfa)')\n",
        "                ax_roc.set_ylabel('Probability of Detection (Pd)')\n",
        "                ax_roc.set_title(f'ROC Curve for {parameter.name} = {value}')\n",
        "                ax_roc.set_xlim([0, 1])\n",
        "                ax_roc.set_ylim([0, 1])\n",
        "                ax_roc.grid(True)\n",
        "                ax_roc.legend(loc='lower right', fontsize=8, frameon=True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            if plot_histogram:\n",
        "                nr_statistics = len(results.keys())\n",
        "                fig = plt.figure(figsize=(18, 7))\n",
        "                nr_cols = np.min([nr_statistics + 1, 6])\n",
        "                nr_rows = int(np.ceil((nr_statistics + 1) / (nr_cols)))\n",
        "\n",
        "                gs = fig.add_gridspec(nr_rows, nr_cols)\n",
        "                axs_hist = []\n",
        "                for gsi in gs:\n",
        "                    axs_hist.append(fig.add_subplot(gsi))\n",
        "\n",
        "                legend_handles_hist = []\n",
        "                legend_labels_hist = []\n",
        "                for idx, key in enumerate(results.keys()):\n",
        "                    ax_hist = axs_hist[idx]\n",
        "                    # Use the last run's h0, h1, threshold for this key\n",
        "                    ax_hist.hist(np.array(all_metrics[key]['H0']), bins=30, alpha=0.6, label='H0', color='tab:blue', density=True)\n",
        "                    ax_hist.hist(np.array(all_metrics[key]['H1']), bins=30, alpha=0.6, label='H1', color='tab:orange', density=True)\n",
        "                    ax_hist.axvline(np.array(results[key]['Threshold'][-1]), color='k', linestyle='--', label='Threshold')\n",
        "                    ax_hist.set_title(f'{key}', fontsize=10)\n",
        "                    ax_hist.set_xlabel(f'{key} Statistic', fontsize=8)\n",
        "                    ax_hist.set_ylabel('Density', fontsize=8)\n",
        "                    ax_hist.grid(True)\n",
        "                    ax_hist.tick_params(axis='both', which='major', labelsize=8)\n",
        "                    ax_hist.tick_params(axis='both', which='minor', labelsize=6)\n",
        "                    if idx == 0:\n",
        "                        # Only collect legend handles/labels from the first subplot\n",
        "                        handles, labels = ax_hist.get_legend_handles_labels()\n",
        "                        legend_handles_hist = handles\n",
        "                        legend_labels_hist = labels\n",
        "\n",
        "                axs_hist[-1].axis('off')\n",
        "                axs_hist[-1].legend(legend_handles_hist, legend_labels_hist, loc='upper center', fontsize=10, frameon=True)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        if plot_statistics:\n",
        "            nr_statistics = len(results.keys())\n",
        "            fig = plt.figure(figsize=(18, 7))\n",
        "            nr_cols = np.min([nr_statistics + 1, 6])\n",
        "            nr_rows = int(np.ceil((nr_statistics + 1) / (nr_cols)))\n",
        "\n",
        "            gs = fig.add_gridspec(nr_rows, nr_cols)\n",
        "            axs = []\n",
        "            for gsi in gs:\n",
        "                axs.append(fig.add_subplot(gsi))\n",
        "\n",
        "            legend_handles = []\n",
        "            legend_labels = []\n",
        "            for idx, key in enumerate(results.keys()):\n",
        "                ax = axs[idx]\n",
        "                h0_line, = ax.plot(parameter.variation, results[key]['H0'], marker='o', label=f'H0 avg')\n",
        "                h1_line, = ax.plot(parameter.variation, results[key]['H1'], marker='s', label=f'H1 avg')\n",
        "\n",
        "                # Plot min and max values as error bars\n",
        "                h0_line_max = ax.errorbar(parameter.variation, results[key]['H0'], yerr=[np.zeros_like(results[key]['H0']), np.array(results[key]['H0_max']) - np.array(results[key]['H0'])], fmt=' ', color='tab:blue', alpha=0.5, capsize=3, label=f'H0 max')\n",
        "                h1_line_min = ax.errorbar(parameter.variation, results[key]['H1'], yerr=[np.array(results[key]['H1']) - np.array(results[key]['H1_min']), np.zeros_like(results[key]['H1'])], fmt=' ', color='tab:orange', alpha=0.5, capsize=3, label=f'H1 min')\n",
        "                threshold_line, = ax.plot(parameter.variation, results[key]['Threshold'], 'k--', linewidth=1.0, label='Decision Threshold')\n",
        "\n",
        "                ax.set_xlabel(parameter.xlabel, fontsize=8)\n",
        "                ax.set_ylabel(f'{key} Statistic', fontsize=8)\n",
        "                ax.set_title(f'{key}', fontsize=10)\n",
        "                if parameter.xticks is not None:\n",
        "                    ax.set_xticks(parameter.xticks)\n",
        "                ax.grid(True)\n",
        "                ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "                ax.tick_params(axis='both', which='minor', labelsize=6)\n",
        "                # Collect legend handles/labels from the first subplot only\n",
        "                if idx == 0:\n",
        "                    legend_handles = [h0_line, h0_line_max, h1_line, h1_line_min, threshold_line]\n",
        "                    legend_labels = ['H0 avg', 'H0 max', 'H1 avg', 'H1 min', 'Decision Threshold']\n",
        "\n",
        "            axs[-1].axis('off')\n",
        "            axs[-1].legend(legend_handles, legend_labels, loc='upper center', fontsize=10, frameon=True)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Plot Pd vs parameter\n",
        "        plt.figure(figsize=(5, 4))\n",
        "        for key in curve_styles.keys():\n",
        "            plt.plot(parameter.variation, results[key]['Pd'], curve_styles[key]['fmt'], linewidth=curve_styles[key]['linewidth'], markerfacecolor=curve_styles[key]['markerfacecolor'], markersize=curve_styles[key]['markersize'], label=curve_styles[key]['label'])\n",
        "\n",
        "        plt.ylim(0, 1)\n",
        "        plt.xlabel(parameter.xlabel)\n",
        "        plt.ylabel('Probability of Detection (Pd)')\n",
        "        plt.title('Pd vs ' + parameter.xlabel)\n",
        "        plt.legend(fontsize=8)\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}